%author: Mario Schroeck <mario.schroeck@roma3.infn.it>
%author: Bartosz Kostrzewa <bartosz_kostrzewa@fastmail.com>
%date: 04/2015
%date: 06/2017, 12/2017, 06/2018, 08/2019, 05/2022, 05/2023

\subsection{QUDA: A library for QCD on GPUs}\label{subsec:quda}


The QUDA \cite{Clark:2009wm, Babich:2011np, Strelchenko:2013vaa} interface provides access to QUDA's solvers both in the HMC~\cite{Kostrzewa:2022hsv} and as a solver interface for propagator production.

\subsubsection{Design goals of the interface}
The QUDA interface has been designed with the following goals in mind, sorted by priority:
\begin{enumerate}
	\item \emph{Safety.} Naturally, highest priority is given to the correctness of the output of the interface. 
	This is trivially achieved by always checking the final residual on the CPU with the default tmLQCD routines.
\item \emph{Ease of use.} Within the operator declarations of the input file (between {\ttfamily BeginOperator} and {\ttfamily EndOperator}) a simple flag {\ttfamily UseExternalInverter} is introduced which, when set to {\ttfamily quda}, will let QUDA perform the inversion of that operator. The operators {\ttfamily TMWILSON, WILSON, DBTMWILSON} and {\ttfamily CLOVER, DBCLOVER} are supported. 
    Within the monomial declarations of the input file (between {\ttfamily BeginMonomial} and {\ttfamily EndMonomial}) the same flag can be used to offload solves for the \texttt{DET, DETRATIO, CLOVERDET, CLOVERDETRATIO, RAT, RATCOR, NDRAT, NDRATCOR, NDCLOVERRAT} and \texttt{NDCLOVERRATCOR} monomials in the HMC.
    Further, the flag {\ttfamily UseExternalLibrary} is introduced which, when set to {\ttfamily quda}, will let QUDA perform the force calculation for the given monomial with support currently limited to {\ttfamily GAUGE, CLOVERDET, CLOVERDETRATIO}.
	\item \emph{Minimality.} Minimal changes in the form of {\ttfamily \#ifdef QUDA} precompiler directives to the tmLQCD code base. The main bulk of the interface lies in a single separate file {\ttfamily quda\_interface.c} (with corresponding header file). The QUDA interface is entered .
	\item \emph{Performance.} The higher priority of the previous items results in small performance detriments. In particular:
	\begin{itemize}
		\item tmLQCD's $\theta$-boundary conditions are not compatible with QUDA's 8 and 12 parameter reconstruction of the gauge fields (as of QUDA-1.1.0). Therefore reconstruction/compression is deactivated by default, although it may be activated via the input file, see below.
    \item The state of the host and device gauge fields is kept synchronised automatically and the gauge field is only transferred if it changes. This tracking of the state also applies to the clover field and the MG preconditioner setup.
	\end{itemize}
\end{enumerate}


\subsubsection{Installation}
If not already installed, you have to install QUDA first. Download the most recent version from \url{http://lattice.github.io/quda/}. Note that QUDA version $\geq 0.7.0$ is required (chiral gamma basis).

QUDA can be installed without any dependencies, consider, e.g., the following minimal configuration:

\begin{verbatim}
cmake \
  -DQUDA_DIRAC_STAGGERED=OFF \
  -DQUDA_DIRAC_DOMAIN_WALL=OFF \
  -DQUDA_DIRAC_WILSON=ON \
  -DQUDA_DIRAC_CLOVER=ON \
  -DQUDA_DIRAC_TWISTED_MASS=ON \
  -DQUDA_DIRAC_TWISTED_CLOVER=ON \
  -DQUDA_DIRAC_NDEG_TWISTED_MASS=ON \
  -DQUDA_DYNAMIC_CLOVER=ON \
  -DQUDA_MPI=ON \
  -DQUDA_INTERFACE_MILC=OFF \
  -DQUDA_INTERFACE_QDP=ON \ 
  -DQUDA_MULTIGRID=ON \
  -DQUDA_GPU_ARCH=sm_37 \
  ${path_to_quda}
\end{verbatim}
where {\ttfamily \$CUDADIR} and {\ttfamily \$MPI\_PATH} have to be set appropriately.
{\ttfamily \$QUDADIR} is your choice for the installation directory of QUDA.
Note that for Wilson clover quarks, you should set \texttt{-DQUDA\_DYNAMIC\_CLOVER=OFF}, whereas the opposite is strictly necessary for twisted mass clover quarks, which means that you will require two QUDA and tmLQCD builds for the time being if you intend to work with both actions.
Note also that if you want to use QUDA in a scalar build of tmLQCD, you should remove the lines {\ttfamily --enable-multi-gpu} and {\ttfamily --with-mpi=\$MPI\_PATH} in the configuration (and probably you want to replace the MPI compilers).
In order to profit from QUDA's autotuning functionality, set the environment variable {\ttfamily QUDA\_RESOURCE\_PATH} to a directory of your choice.
Every time that you update your QUDA installation or change some of the many QUDA environment variables, the files in the directory will have to be deleted or a new directory chosen.
It is convenient to base the directory dynamically on the head git commit of your QUDA source tree as well as the value of the {\ttfamily QUDA\_ENABLE\_GDR} environment variable.
There may be other environment variables which make one set of auto-tuning results incompatible with another.

Once QUDA is installed, a minimal configuration of tmLQCD could look like, e.g.,
\begin{verbatim}
./configure CC=mpicc \
--prefix=$TMLQCDDIR \
--with-limedir=$LIMEDIR \
--with-lapack=<linker-flags> \
--enable-mpi \
--with-mpidimension=4 \
CXX=mpiCC \
--with-qudadir=$QUDADIR \
--with-cudadir=${CUDADIR}/lib
\end{verbatim}
Note that a {\ttfamily C++} compiler is required for linking against the QUDA library, therefore set {\ttfamily CXX} appropriately. {\ttfamily \${QUDADIR}} is where you installed QUDA in the previous step and {\ttfamily \${CUDADIR}} is required again for linking.

\subsubsection{QUDA versions}

If you need a version of QUDA after https://github.com/lattice/quda/commit/50864ffde1bd8f46fd4a2a2b2e6d44a5a588e2c2 you nee to configure with 
\begin{verbatim}
  --enable-quda_experimental=yes
\end{verbatim}

If you need a version of QUDA before \url{https://github.com/lattice/quda/commit/fd50676db06fc36efb3a791a3059c57cca70bb55} you need to add in the configuration script the option
\begin{verbatim}
  --enable-quda_fermionic_forces=no
\end{verbatim}
so that the wrapper to the QUDA fermionic forces is not compiled,
thus if \texttt{--enable-quda_fermionic_forces=no} setting {\ttfamily UseExternalLibrary=yes} in the inputfile for the {\ttfamily  CLOVERDET, CLOVERDETRATIO} monomials
is not supported and tmLQCD will stop with an error.


\subsubsection{Usage}
Any main program that reads and handles the operator declaration from an input file can easily be set up to use the QUDA inverter by setting the {\ttfamily UseExternalInverter} flag to {\ttfamily quda}. For example, in the input file for the {\ttfamily invert} executable, add the flag to the operator declaration as
\begin{verbatim}
BeginOperator TMWILSON
  2kappaMu = 0.05
  kappa = 0.177
  UseEvenOdd = yes
  Solver = CG
  SolverPrecision = 1e-14
  MaxSolverIterations = 1000
  UseExternalInverter = quda
EndOperator
\end{verbatim}
and the operator of interest will be inverted using QUDA. The initialization of QUDA is done automatically within the operator initialization,  the QUDA library should be finalized by a call to {\ttfamily \_endQuda()} just before finalizing MPI. When you use the QUDA interface for work that is being published, don't forget to cite \cite{Clark:2009wm, Babich:2011np, Strelchenko:2013vaa}.

At the operator level as well as for fermionic monomials in the HMC, the following additional parameters affect QUDA solvers:
\begin{itemize}
  \item \texttt{Solver}: Type of solver to employ. Possible values for QUDA:
    \begin{itemize}
      \item \texttt{CG} (conjugate gradient)
      \item \texttt{CGMMS} (multishift conjugate gradient)
      \item \texttt{CGMMSND} (multishift conjugate gradient for the non-degenerate operator)
      \item \texttt{BICGSTAB} (bi-conjugate gradient, useful for non-twisted Wilson and Wilson-clover fermions)
      \item \texttt{MG} (multigrid-preconditioned GCR)
    \end{itemize}
    Unlike the tmLQCD-native solvers, QUDA solvers are always run in mixed-precision and there is no differentiation between the names of mixed- and fixed-precision solvers, the inner solver precision is instead set by \texttt{UseSloppyPrecision}.
    (default: \texttt{CG})
  \item \texttt{UseCompression}: Employ gauge compression. Possible values: 
    \begin{itemize}
      \item $8$ (use 8-real gauge compression)
      \item $12$ (use two-row gauge compression)
      \item $18$ (disable gauge compression)
    \end{itemize}
    Since tmLQCD always uses twisted fermionic boundary conditions, the default $18$ is recommended unless you know what you are doing. Note that if the global fermionic boundary condition parameters \texttt{Theta[X,Y,Z,T]} are set non-zero, this setting is overriden by default. See \texttt{FermionBC} to force a particular type of fermionic boundary conditions in QUDA. Note that the tmLQCD residual check will fail if compression is enabled and non-twisted boundary conditions are forced.
  \item \texttt{UseSloppyPrecision}: Employ this precision for the inner solver of a mixed-precision solver (outer solvers are always run in double precision). Possible values:
    \begin{itemize}
      \item \texttt{no} / \texttt{double} (inner solver running in double precision)
      \item \texttt{single} / \texttt{float} (inner solver running in single precision)
      \item \texttt{half} (inner solver running in half precision)
    \end{itemize}
    (string, default: \texttt{double})
  \item \texttt{RefinementPrecision}: When the operator or monomial uses the multishift (\texttt{cgmms[nd]}) solver and offloads to QUDA, this parameter sets the inner solver precision of shift-by-shift refinement solves. In practice, one might set \texttt{UseSloppyPrecision = single} and \texttt{RefinementPrecision = half}. This will iterate the residuals in the multishift solver up to single precision and then refine each solution using a double-half mixed-precision CG.
\end{itemize}

In additition, for the gauge monomial, the parameter \texttt{UseExternalLibrary = quda} can be used to offload the gauge force to QUDA.

Finally, for the \texttt{GRADIENTFLOW} online measurement, the parameter \texttt{UseExternalLibrary = quda} will offload the gradient flow to QUDA.

\subsubsection{General settings}
Some properties of the QUDA interface can be configured via the {\ttfamily ExternalInverter} section.
\begin{itemize}
  \item \texttt{FermionBC} Forces twisted ({\ttfamily theta}), periodic ({\ttfamily pbc}) or antiperiodic ({\ttfamily apbc}) temporal quark field boundary conditions irrespective of what has been set for \texttt{ThetaT}. This setting exists because at the time of writing (2017.12.28), there seems to be a bug or incompatibility in QUDA which causes (anti-)periodic boundary conditions with gauge compression to produce incorrect propagators. Use with care as the residual check using tmLQCD operators will suggest a non-converged residual.
  \item \texttt{Pipeline} The pipeline length for fused operations in some solvers (for the tmLQCD QUDA interface, at the time of writing in May 2023, this is just GCR). (positive integer, default: $0$)
  \item \texttt{gcrNkrylov} Maximum size of Krylov space used by the solver. (positive integer, default: 10)
  \item \texttt{ReliableDelta} Reliable update tolerance. (positive float, default: 0.001)
\end{itemize}

\subsubsection{More advanced settings}
To achieve higher performance you may choose single (default) or even half precision as sloppy precision for the inner solver of the mixed precision inverter with reliable updates. After {\ttfamily BeginOperator} and before {\ttfamily EndOperator} set {\ttfamily UseSloppyPrecision = double|single|half}.
The MG-preconditioned GCR solver only works in double-single mixed precision, but the null vectors are stored in half precision as recommended by Kate Clark.

To activate compression of the gauge fields (in order to save bandwidth and thus to achieve higher performance), set {\ttfamily UseCompression = 8|12|18} within {\ttfamily BeginOperator} and {\ttfamily EndOperator}. 
The default is 18 which corresponds to no compression. 
Note that if you use compression, trivial (anti)periodic boundary conditions will be applied to the gauge fields, instead of the default $\theta$-boundary conditions. 
As a consequence, the residual check on tmLQCD side will fail. 
Moreover, compression is not applicable when using general $\theta$-boundary conditions in the spatial directions. 
If trying to do so, compression will be de-activated automatically and the user gets informed via the standard output.
The \texttt{FermionBC} setting can be used to force particular temporal boundary conditions to be applied to the gauge field in the Dirac operator.

\subsubsection{Functionality}
The QUDA interface can currently be used to invert {\ttfamily TMWILSON, WILSON, DBTMWILSON} and {\ttfamily CLOVER} within a 4D multi-GPU (MPI) parallel environment with CG, BICGSTAB or MG-preconditioned GCR. QUDA uses even-odd preconditioning, if wanted ({\ttfamily UseEvenOdd = yes}), and the interface is set up to use a mixed precision solver by default. For more details on the QUDA settings check the function {\ttfamily \_initQuda()} in {\ttfamily quda\_interface.c}.

\subsubsection{QUDA-MG interface}
The interface has support for the QUDA Multigrid (MG) solver implementation and allows a number of parameters to be adjusted in order to tune the MG setup.
The defaults for these parameters follow the recommendations of \url{https://github.com/lattice/quda/wiki/Multigrid-Solver}, which also provides useful hints for further tuning.
The K-cycle is used by default and there is currently no user-exposed option for changing this.

The MG-preconditioned GCR solver is selected as follows:
\begin{verbatim}
BeginOperator TMWILSON
  2kappaMu = 0.05
  kappa = 0.177
  UseEvenOdd = yes
  Solver = mg
  SolverPrecision = 1e-18
  MaxSolverIterations = 200
  UseExternalInverter = quda
  UseSloppyPrecision = single
EndOperator
\end{verbatim}

\paragraph{Basic MG parameters:} The MG setup can be tuned using the following parameters in the \texttt{BeginExternalInverter QUDA} section:
\begin{itemize}
  \item{ \texttt{MGNumberOfLevels}: Number of levels to be used in the MG, $3$ is usually ideal but $2$ can be similarly efficient depending on the quark mass (positive integer, default: $3$) }
  \item{ \texttt{MGSetupSolver}: Solver used for generating null vectors. Possible values:
    \begin{itemize}
      \item \texttt{CG} (conjugate gradient)
      \item \texttt{CGNE} (conjugate gradient normal equations)
      \item \texttt{CGNR} (conjugate gradient normal residuals)
      \item \texttt{CACG} (communication-avoiding conjugate gradient)
      \item \texttt{CACGNE} (communication-avoiding conjugate gradient normal equations)
      \item \texttt{CACGNR} (communication-avoiding conjugate gradient normal residuals)
      \item \texttt{BiCGstab} (stabilised bi-conjugate gradient)
    \end{itemize}
    Usage of \texttt{BiCGstab} may be recommended for Wilson or clover Wilson quarks while \texttt{CG} should always be used for twised mass or twisted clover quarks. \texttt{CACG} may be beneficial at the strong-scaling limit, see also \texttt{MGSetupCABasisType} and \texttt{MGSetupCABasisSize}. At the time of writing (May 2023), this cannot be set on a per-level basis so the effect of the communication-avoiding setup solver on the setup time may be limited or even detrimental.
        (default: \texttt{CG})}
  \item{ \texttt{MGVerbosity}: Verbosity of the solver on a per-level basis. Possible values:
    \begin{itemize}
      \item \texttt{silent} (suppress any additional output)
      \item \texttt{summarize} (summary information such as number of iterations and achieved residual in the respective smoother or solver)
      \item \texttt{verbose} (verbose information such as per-iteration residual printing of the respective solver or smoother)
      \item \texttt{debug\_verbose} (extremely verbose information at each iteration of the respective smoother or solver)
    \end{itemize}
    (comma-separated list of strings, default: \texttt{silent} on all levels)}
  \item{ \texttt{MGSetupSolverTolerance}: Relative target residual (unsquared!) during setup phase on a per-level basis. (comma-separated list of positive floats, default: $1\cdot10^{-6}$ on all levels) }
  \item{ \texttt{MGSetupMaxSolverIterations}: Maximum number of iterations during setup phase on a per-level basis. (comma-separated list of positive integers, default: $1000$ on all levels) }
  \item{ \texttt{MGCoarseSolverTolerance}: Unsquared relative target residual on the coarse grids on a per-level basis. (comma-separated list of positive floats, default: $0.25$ on all levels) }
  \item{ \texttt{MGNumberOfVectors}: Number of null vectors to compute on a per-level basis. (comma-separated list of integers, possible values $\left[ 24, 32 \right]$, default: $24$ on all levels)}
  \item{ \texttt{MGCoarseMaxSolveriterations}: Maximum number of iterations on coarse grids on a per-level basis. (comma-separated list of positive integers, default: $75$ on all levels) }
  \item{ \texttt{MGEnableSizeThreeBlocks}: Historically and by default, QUDA has had limited support for size $3$ aggregates. If set to \emph{yes}, the automatic blocking algorithm will attempt to use them for lattice extents divisible by $3$ when the local lattice extent at a given level is smaller than $16$ aggregate sites. This may require you to instantiate the necessary block sizes in QUDA (see comments below). In recent (2022 and later) commits of the QUDA \texttt{develop} branch, this can usually be set to \emph{yes} without having to modify QUDA. (boolean \emph{yes} or \emph{no}, default: \emph{no}) }
  \item{ \texttt{MGBlockSizes[X,Y,Z,T]}: Aggregate sizes on each level. When these are set non-zero for a given lattice dimension and a given level, the automatic blocking algorithm for that dimension and level is overridden and the specified blockings are forced. When the required aggregate sizes are not instantiated in QUDA, the setup phase will fail with an informative error message. (comma-separated list of positive integers, for a three level solver, for example, two numbers needs to be specified: aggregate size on the finest and intermediate level, default: \texttt{0, 0})}
\end{itemize}

\paragraph*{Automatic blocking:} If no blocking is specified manually, the aggregation parameters are set automatically as follows:
\begin{itemize}
  \item{ A default block size of $4$ is attempted if the MPI-partitioned local lattice extent on the current level is even and larger than or equal to $16$ lattice sites. }
  \item{ If the number of lattice sites in a given direction on the current level is even and smaller than $16$, a block size of $2$ is used. }
  \item{ The option \texttt{MGEnableSizeThreeBlocks} can be set to \texttt{yes}. Then, for levels coarser than the fine grid, extents smaller than $16$ and divisible by $3$, a block size of $3$ will be used. This might require the addition of instantiations of block sizes to QUDA in the restrictor and transfer operator. (\texttt{lib/restrictor.cu} and \texttt{lib/transfer\_util.cu}) }
  \item{ In all other cases, aggregation is disabled for this direction and level. This includes, for instance, extents divisible by primes other than $2$ or $3$. This means that if you need blocks of $5$ or $7$ for geometry reasons, you will need to choose a setup manually.}
\end{itemize}

A typical MG setup might look like this for twisted mass clover quarks: 

\begin{verbatim}
BeginExternalInverter QUDA
  MGNumberOfLevels = 3
  MGSetupSolver = cg
  MGSetupSolverTolerance = 1e-6, 1e-6
  MGSetupMaxSolverIterations = 1000, 1000
  MGCoarseSolverTolerance = 0.25, 0.25, 0.25
  MGCoarseSolverIterations = 75, 75, 75
  MGSmootherTolerance = 0.25, 0.25, 0.25
  MGSmootherPreIterations = 2, 2, 2
  MGSmootherPostIterations = 4, 4, 4
  MGOverUnderRelaxationFactor = 0.85, 0.85, 0.85
  MGCoarseMuFactor = 1.0, 1.0, 12.0
  MGNumberOfVectors = 24, 24, 32
  MGRunVerify = yes
  MGEnableSizeThreeBlocks = no
EndExternalInverter
\end{verbatim}

Alternatively, a blocking can be specified manually:

\begin{verbatim}
BeginExternalInverter QUDA
  MGNumberOfLevels = 3
  MGBlockSizesX = 4, 3
  MGBlockSizesY = 4, 3
  MGBlockSizesZ = 6, 4
  MGBlockSizesT = 6, 4
  MGSetupSolver = cg
  MGSetupSolverTolerance = 1e-6, 1e-6
  MGSetupMaxSolverIterations = 1000, 1000
  MGCoarseSolverTolerance = 0.25, 0.25, 0.25
  MGCoarseSolverIterations = 75, 75, 75
  MGSmootherTolerance = 0.25, 0.25, 0.25
  MGSmootherPreIterations = 2, 2, 2
  MGSmootherPostIterations = 4, 4, 4
  MGOverUnderRelaxationFactor = 0.85, 0.85, 0.85
  MGCoarseMuFactor = 1.0, 1.0, 12.0
  MGRunVerify = yes
EndExternalInverter
\end{verbatim}

Note that at the time of writing (2017.12.28), only double-single mixed-precision is supported for the MG-preconditioned GCR solver and the solve will abort if a double-half precision solve is attempted. It should be noted that the null vectors are kept in half precision as recommended, however.

\paragraph{Further basic MG parameters:}
\begin{itemize}
  \item{ \texttt{MGSmootherTolerance}: unsquared relative target residual of the smoother on a per-level basis. (comma-separated list of positive floats, default: $0.25$ on all levels) }
  \item{ \texttt{MGSmootherPreIterations}: number of smoothing steps before coarse grid correction on a per-level basis. (comma-separated list of zero or positive integers, default: $0$ on all levels)}
  \item{ \texttt{MGSmootherPostIterations}: number of smoothing steps after prolongation on a per-level basis. (comma-separated list of zero or positive integers, default: $4$ on all levels)}
  \item{ \texttt{MGOverUnderRelaxationFactor}: Over- or under-relaxation factor on a per-level basis. (comma-separated list of positive floats, default: $0.85$ on all levels)}
  \item{ \texttt{MGCoarseMuFactor}: Scaling factor for twisted mass on a per-level basis, accelerates convergence and reduces condition number of coarse grid solves. From experience it seems that it's reasonable to set this $>1.0$ only on the coarsest level, but sometimes it might also help on intermediate levels. If running with twisted mass, this should always be set and tuned for maximum efficiency. When using coarse-grid deflation (see \texttt{MGEigSolverRequireConvergence}), this should usually be set to $1.0$ on all levels. (comma-separated list of positive floats, usually $ > 1.0$, default: $1.0$ on all levels).}
  \item{ \texttt{MGSetup2KappaMu}: The value of $2\kappa\mu$ which should be used during the MG setup process. This is important in the HMC for standard twisted mass fermions, for example, because the setup should always be performed with the smallest quark mass to be employed in a simulation and it might be that a monomial with a heavier twisted quark mass is the first to call to MG and to thus trigger the setup. Generally this is set to the target light twisted quark mass. Setting this to $0.0$ implies that it is ignored. (float, default: $0.0$) }
  \item{ \texttt{MGReuseSetupMuThreshold}: When the twisted quark mass is changed between solves using the MG solver, the MG setup is usually \emph{updated} for this new $\mu$ value. One can attempt to reuse the MG setup for solves with different $\mu$ values up to this threshold, i.e., when the condition $x < 2\kappa\cdot|\mu_\mathrm{old} - \mu_\mathrm{new}|$ holds. (positive float, default: \texttt{2*DBL\_EPSILON})}
  \item{ \texttt{MGRefreshSetupMDUThreshold}: When the MG is used in the HMC, the MG setup must be regularly refreshed by running a few iterations of the setup solver on the current set of approximate null vectors in order to evolve these with the changing gauge field. A good rule of thumb is to perform this setup refresh about twice per coarsest time step. In other words: for a trajectory length $\tau$ and $N$ integration steps on the coarsest time scale, the refresh should be performed at intervals of $(\tau/(2N)-\epsilon)$ MDUs, where $\epsilon$ is a small number to make sure that the threshold is hit at every half-step of the integrator. (positive float, default: \texttt{2*DBL\_EPSILON})}
  \item{ \texttt{MGRefreshSetupMaxSolverIterations}: Number of setup iterations to be performed on a per-level basis when a setup refresh is required, see also \texttt{MGRefreshSetupMDUThreshold} above. This must be specified for the HMC and a value between 20 and 35 is usually sufficient. (comma-separated list of positive integers, default: $0$ on all levels}
  \item{ \texttt{MGResetSetupMDUThreshold}: When the MG is used in the HMC, the MG setup must be reset (instead of being refreshed) if the gauge configuration changes significantly at some point. This is the case, for example, when a trajectory is rejected. This value must always be set and a possible choice is to set it to the trajectory length. Note that there is an interaction with the preprocessor constant \texttt{TM\_GAUGE\_PROPAGATE\_THRESHOLD}, the default value of which is $3.0$. This is because when a trajectory is rejected, an internal variable which tracks the state of the gauge field is incremented by \texttt{TM\_GAUGE\_PROPAGATE\_THRESHOLD} to ensure that everything that depends on the gauge field is properly updated. This means that if the trajectory length $\tau \geq 3.0$, one should ignore the recommendation and set $x < 3.0$. In general, any value significantly larger than \texttt{MGRefreshSetupMDUThreshold} should be fine. (positive float, default: \texttt{2*DBL\_EPSILON}) }
  \item{ \texttt{MGRunVerify}: Check GPU coarse-grid operators against CPU reference implementation and verify Galerkin projectors during setup phase. This is usually fast enough to always be performed, although sometimes it seems to fail even though the setup works fine. (boolean \emph{yes} or \emph{no}, default: \emph{no}) }
  \item{ \texttt{MGRunObliqueProjectionCheck}: Measure how well the null vector subspace adjusts the low eigenmode subspace. (boolean \emph{yes} or \emph{no}, default: \emph{no}) }
  \item{ \texttt{MGLowModeCheck}: Measure how well the null vector subspace overlaps with the low eigenmode subspace. (boolean \emph{yes} or \emph{no}, default: \emph{no}) }
\end{itemize}

The default parameters and an automatically determined aggregation will generally produce a solver which will outperform mixed-precision CG by about one to two orders of magnitude (at the physical point) as long as the value for \texttt{MGCoarseMuFactor} on the coarsest grid is tuned appropriately. Further refinements are possible and improvements by up to another order of magnitude are achievable through smart choices for parameters related to the smoother, the number of vectors used on each level and the tolerances related to the smoother and coarse-grid solvers. These details are dependent on the target quark mass as well as the underlying ensemble and they may be dependent on the particular configuration (for certain ensembles, time to solution fluctuates quite strongly depending on the point in the molecular dynamics history). Finally, the tuning of the algorithmic parameters depends on the characteristics of the particular machine as one may compensate, for example, a poorly performing coarsest-grid operator through more iterations on the intermediate and fine grids. This kind of compensation will lead to a setup which is algorithmically less efficient (more matrix-vector products) but may have a lower overall time-to-solution.

For use in the HMC, a typical MG setup might look something like:

\begin{verbatim}
BeginExternalInverter QUDA
  EnablePinnedMemoryPool = yes 
  EnableDeviceMemoryPool = no

  Pipeline = 16
  gcrNkrylov = 24
  MGRunVerify = no
  MGNumberOfLevels = 3
  MGNumberOfVectors = 24, 32
  MGSetupSolver = cg
  MGSetup2KappaMu = 0.00014900994792
  MGVerbosity = silent, silent, silent
  MGSetupSolverTolerance = 5e-7, 5e-7
  MGSetupMaxSolverIterations = 1500, 1500
  MGCoarseSolverType = gcr, gcr, cagcr

  MGCoarseMuFactor = 1.0, 1.75, 130.0
  MgCoarseSolverTolerance = 0.1, 0.25, 0.25
  MGCoarseMaxSolverIterations = 15, 10, 10

  MGSmootherType = cagcr, cagcr, cagcr
  MGSmootherTolerance = 0.15, 0.15, 0.1

  MGSmootherPreIterations = 0, 0, 1
  MGSmootherPostIterations = 1, 3, 1

  MGBlockSizesX = 4, 4
  MGBlockSizesY = 4, 4
  MGBlockSizesZ = 4, 2
  MGBlockSizesT = 4, 2

  MGOverUnderRelaxationFactor = 0.95, 0.95, 0.95
  MGRefreshSetupMDUThreshold = 0.03124
  MGRefreshSetupMaxSolverIterations = 35, 35
  MGResetSetupMDUThreshold = 1.0
EndExternalInverter
\end{verbatim}

\paragraph{Advanced MG parameters:}
\begin{itemize}
  \item{ \texttt{MGCoarseSolverType}: The type of solver to be used on the intermediate and coarse grids. While this has to be specified for all levels, it will be ignored on the fine grid. Possible values:
    \begin{itemize}
      \item \texttt{cagcr} (communication-avoiding generalised conjugate residual)
      \item \texttt{gcr} (generalised conjugate residual)
      \item \texttt{mr} (minimal residual)
    \end{itemize}
    The recommendation is to use \texttt{gcr} on all intermediate and \texttt{cagcr} on the coarsest level.
    (comma-separated list of strings, default: \texttt{gcr} on all levels)}
  \item{ \texttt{MGSmootherType}: The type of algorithm used as a smoother on a per-level basis. Possible values:
    \begin{itemize}
      \item \texttt{cagcr} (communication-avoiding generalised conjugate residual)
      \item \texttt{mr} (minimal residual)
    \end{itemize}
    It is often beneficial to employ \texttt{cagcr} on all levels as a smoother.
    (comma-separated list of strings, default: \texttt{cagcr} on all levels)}
\end{itemize}
The fine-tuning parameters for the communication-avoiding solver are the same whether it is used as a smoother or coarse-grid solver.
\begin{itemize}
  \item{ \texttt{MGCoarseSolverCABasisType} and \texttt{MGSmootherSolverCABasisType}: The type of basis to use for the communication-avoiding solver on a per-level basis. Ignored unless \texttt{cagcr} is set for \texttt{MGCoarseSolverType} (\texttt{MGSmootherType}) for the given level. Possible values:
  \begin{itemize}
    \item \texttt{power} (power basis)
    \item \texttt{chebyshev} (Chebyshev basis)
  \end{itemize}
    (comma-separated list of strings, default: \texttt{power} on all levels)}
  \item{ \texttt{MGCoarseSolverCABasisSize}: The size of the polynomial basis to use if a communication-avoiding solver is used on a per-level basis. Ignored unless \texttt{cagcr} is set for \texttt{MGCoarseSolverType} for the given level. Values larger than $4$ require \texttt{chebyshev} to be set for the corresponding \texttt{MGCoarseSolverCABasisType}. There is no corresponding parameter for the smoother. (comma-separated list of positive integers, default: $4$ on all levels)}
  \item{ \texttt{MGCoarseSolverCABasisLambdaMin} and \texttt{MGSmootherSolverCABasisLambdaMin}: Minimum eigenvalue specified for each level if the Chebyshev basis is used for the corresponding \texttt{MGCoarseSolverCABasisType} (\texttt{MGSmootherSolverCABasisType}). The default value of $0.0$ is recommened for safety. (comma-separated list of postitive floats, default: $0.0$ on all levels)}
  \item{ \texttt{MGCoarseSolverCABasisLambdaMax} and \texttt{MGSmootherSolverCABasisLambdaMax}: Maximum eigenvalue specified for each level if the Chebyshev basis is used for the corresponding \texttt{MGCoarseSolverCABasisType} (\texttt{MGSmootherSolverCABasisType}). The default value of $-1.0$ triggers power iterations which estimate this automatically. (comma-separated list of positive floats, default: $-1.0$ on all levels)}
\end{itemize}
The fine-tuning parameters for the communication-avoiding solvers used in the MG setup are nominally the same as for the coarse-grid solver and smoother but this may diverge in principle as different solvers are employed in the setup.
\begin{itemize}
  \item{ \texttt{MGSetupCABasisType}: The type of polynomial basis to use if a communication-avoiding solver is used in the setup on a per-level basis. Possible values:
  \begin{itemize}
    \item \texttt{power} (power basis)
    \item \texttt{chebyshev} (Chebyshev basis)
  \end{itemize}
    The recommendation is to use \texttt{chebyshev} for stability reasons. (comma-separated list of strings, default: \texttt{power} on all levels)}
  \item{ \texttt{MGSetupCABasisSize}: The size of the polynomial basis to use if a communication-avoiding solver is used in the setup on a per-level basis. Values larger than $4$ require \texttt{chebyshev} to be set for the corresponding \texttt{MGSetupCABasisType}. (comma-separated list of positive integers, default: $4$ on all levels)}
  \item{ \texttt{MGSetupCABasisLambdaMin}: Minimum eigenvalues specified for each level if the Chebyshev basis is used for the corresponding \texttt{MGSetupCABasisType}. The default value of $0.0$ is recommended for safety. (comma-separated list of positive floats, default: $0.0$ on all levels)}
  \item{ \texttt{MGSetupCABasisLambdaMax}: Maximum eigenvalues specified for each level if the Chebyshev basis is used for the corresponding \texttt{MGSetupCABasisType}. The default value of $-1.0$ triggers power iterations which estimate this automatically. (comma-separated list of positive floats, default: $-1.0$ on all levels)}
\end{itemize}

\paragraph{Coarse-grid-deflated MG solver:}
Especially with twisted mass quarks, the coarse-grid operator is very poorly conditioned. One way of improving the convergence of the solver is the scaling of the $\mu$ parameter on the coarsest and intermediate grid as described in \texttt{MGCoarseMuFactor} above. A different and more potent way which pays off for propagator inversions (but is not suitable for the HMC) is to use coarse-grid deflation. In this case, the first $n_\mathrm{ev}$ eigenvectors of the coarse-grid operator are computed and the resulting deflation subspace is used to precondition the system. In this case, experience seems to indicate that \texttt{MGCoarseMuFactor} should be set to $1.0$ everywhere as it is no longer required to improve convergence.
An example for twisted mass Wilson-clover multigrid can be found at \url{https://github.com/lattice/quda/wiki/Twisted-clover-deflated-multigrid} and some further documentation is to be found at \url{https://github.com/lattice/quda/wiki/QUDA's-eigensolvers}.

The options of QUDA's eigensolver are set via the following set of parameters:
\begin{itemize}
  \item{ \texttt{MGUseEigSolver}: Whether or not the eigensolver should be run for the operator on a per-level basis. The most common use-case would be to enable the eigensolver on the coarsest grid. (comma-separated list of booleans \emph{yes} or \emph{no}, default: \emph{no} on all levels)}
  \item{ \texttt{MGEigSolverRequireConvergence}: Require that the eigensolver actually must converge for the requested number of eigenvectors on a per-level basis as specified by \texttt{MGEigSolverNumberOfVectors}. Ignored if the corresponding \texttt{MGUseEigSolver} is set to \texttt{no}. (comma-separated list of booleans, default: \texttt{yes} on all levels)}
  \item{ \texttt{MGEigSolverUseNormOp}: When set to \texttt{yes} on a per-level basis, the eigensolver uses $MM^\dagger$ instead of $M$. (comma-separated list of booleans, default: \texttt{no} on all levels)}
  \item{ \texttt{MGEigSolverUseDagger}: When set to \texttt{yes} on a per-level basis, the eigensolver uses $M^\dagger$ instead of $M$ (or $M^\dagger M$ when \texttt{MGEigSolverUseNormOp} is set to \texttt{yes} on the respective level). (comma-separated list of booleans, defaullt: \texttt{no} on all levels)}
  \item{ \texttt{MGEigSolverType}: Type of eigensolver to use on a per-level basis. Possible values:
    \begin{itemize}
      \item \texttt{tr\_lanczos} (truncated Lanczos)
      \item \texttt{ir\_arnoldi} (implicitly-restarted Arnoldi)
    \end{itemize}
    (comma-separated list of strings, default: \texttt{tr\_lanczos} on all levels)}
  \item{ \texttt{MGEigSolverSpectrum}: Which part of the spectrum the eigensolver should focus on specified on a per-level basis. Possible values:
    \begin{itemize}
      \item \texttt{smallest\_real} (smallest eigenvalues based on their real part)
      \item \texttt{largest\_real} (largest eigenvalues based on their real part)
      \item \texttt{smallest\_imaginary} (smallest eigenvalues based on their imaginary part)
      \item \texttt{largest\_imaginary} (largest eigenvalues based on their imaginary part)
      \item \texttt{smallest\_modulus} (smallest eigenvalues in modulus)
      \item \texttt{largest\_modulus} (largest eigenvalues in modulus)
    \end{itemize}
    For the purpose of deflating the coarse-grid system, the usual choice is \texttt{smallest\_real}.
    (comma-separated list of strings, default: \texttt{smallest\_real} on all levels)}
  \item{ \texttt{MGEigSolverCheckConvergenceInterval}: When using an implicitly-restarted solver, perform this many restarts between convergence checks. (comma-separated list of positive integers, default: $5$ on all levels)}
  \item{ \texttt{MGEigSolverMaxRestarts}: Perform at most this many restarts in the eigensolver specified on a per-level basis. (comma-separated list of positive integers, default: $100$ on all levels)}
  \item{ \texttt{MGEigSolverTolerance}: The tolerance to use in the eigensolver on a per-level basis. (comma-separated list of floats, default: $1.0\cdot10^{-6}$ on all levels)}
  \item{ \texttt{MGEigPreserveDeflationSubspace}: When a coarse-grid-deflated MG setup is updated between a change of the $\mu$ parameter, setting this to \texttt{yes} will preserve the deflation subspace. If set to \texttt{no}, the deflation subspace would be destroyed and regenerated upon a change of $\mu$. (boolean, default: \texttt{yes})}
  \item{ \texttt{MGEigSolverNumberOfVectors}: Number of eigenvectors to compute on a per-level basis. Usually, this is intended for the coarsest level where, depending on the lattice size and physical parameters, numbers between $512$ and $3072$ may be required. On the levels where \texttt{MGEigSolverRequireConvergence = no} is set, this should be set to the value of \texttt{MGNumberOfVectors} on that level. (comma-separated list of positive integers, default: $0$ on all levels)}
  \item{ \texttt{MGEigSolverKrylovSubspaceSize}: Size of the Krylov subspace to be used to be built using the eigenvectors. A good rule of thumb is to set this to a small multiple ($2\times$ or $3\times$) of the corresponding value for \texttt{MGEigSolverNumberOfVectors} for the respective level. On levels where \texttt{MGEigSolverRequireConvergence = no} is set, this should be set to the corresponding value of \texttt{MGNumberOfVectors}. (comma-separated list of positive integers, default: $0$ on all levels)}
  \item{ \texttt{MGEigSolverUsePolynomialAcceleration}: Whether or not to use polynomial acceleration in the eigensolver on a per-level basis. (comma-separated list of booleans, default: \texttt{yes} everywhere)}
  \item{ \texttt{MGEigSolverPolynomialDegree}: Degree of the polynomial used for polynomial acceleration of the eigensolver, specified on a per-level basis. This will be ignored except if \texttt{MGEigSolverUsePolynomialAcceleration = yes} is set on a given level. Values between $50$ to $100$ seem to work well. (comma-separated list of postive integers, default: $100$ on all levels)}
  \item{ \texttt{MGEigSolverPolyMin}: Smallest eigenvalue to suppress via polynomial acceleration. It should not be set too low and only lowered step by step from its default value. Has a large impact on the rate of convergence of the eigensolver when set correctly. Ignored unless \texttt{MGEigSolverUsePolynomialAcceleration = yes} is set on the corresponding level. (comma-separated list of positive real numbers, default: $1.0$ on all levels)}
  \item{ \texttt{MGEigSolverPolyMax}: Largest eigenvalue which should be suppressed via polynomial acceleration. This should be set about 20\% larger than the actual largest eigenvalue of the operator. Ignored unless \texttt{MGEigSolverUsePolynomialAcceleration = yes} is set. (comma-separated list of positive real numbers, default: $5.0$ on all levels)}
\end{itemize}

A typical coarse-grid-deflated setup might look something like:

\begin{verbatim}
BeginExternalInverter QUDA
  EnablePinnedMemoryPool = yes
  EnableDeviceMemoryPool = no
  Pipeline = 16
  gcrNkrylov = 24
  MGRunVerify = no
  MGCoarseMuFactor = 1.0, 1.0, 1.0
  MGNumberOfLevels = 3
  MGNumberOfVectors = 24, 32
  MGSetupSolver = cg
  MGSetup2KappaMu = 0.000200774448
  MGVerbosity = silent, silent, silent
  MGBlockSizesX = 4, 2
  MGBlockSizesY = 4, 2
  MGBlockSizesZ = 4, 2
  MGBlockSizesT = 4, 2
  MGSetupSolverTolerance = 5e-7, 5e-7
  MGSetupMaxSolverIterations = 1500, 1500
  MGCoarseSolverType = gcr, gcr, cagcr
  MgCoarseSolverTolerance = 0.1, 0.1, 0.1
  MGCoarseMaxSolverIterations = 10, 10, 10
  MGSmootherType = cagcr, cagcr, cagcr
  MGSmootherTolerance = 0.1, 0.1, 0.1
  MGSmootherPreIterations = 0, 0, 0
  MGSmootherPostIterations = 2, 2, 2
  MGOverUnderRelaxationFactor = 0.90, 0.90, 0.90

  MGUseEigSolver = no, no, yes
  MGEigSolverType = tr_lanczos, tr_lanczos, tr_lanczos
  MGEigSolverSpectrum = smallest_real, smallest_real, smallest_real
  MGEigPreserveDeflationSubspace = yes
  MGEigSolverNumberOfVectors = 0, 0, 1024
  MGEigSolverKrylovSubspaceSize = 0, 0, 3072
  MGEigSolverRequireConvergence = no, no, yes
  MGEigSolverMaxRestarts = 0, 0, 100
  MGEigSolverTolerance = 1e-4, 1e-4, 1e-4
  MGEigSolverUseNormOp  = no, no, yes
  MGEigSolverUseDagger = no, no, no
  MGEigSolverUsePolynomialAcceleration = no, no, yes
  MGEigSolverPolynomialDegree = 0, 0, 100
  MGEigSolverPolyMin = 0.0, 0.0, 0.01
  MGEigSolverPolyMax = 0.0, 0.0, 3.6
  MGCoarseSolverCABasisSize = 4, 4, 4
EndExternalInverter
\end{verbatim}

In order to determine approppriate values for \texttt{MGEigSolverPolyMin} and \texttt{MGEigSolverPolyMax}, one should run the eigensolver once without polynomial acceleration (\texttt{MGEigSolverUsePolynomialAcceleration = no}) and run tmLQCD at \texttt{DebugLevel = 3} to trigger the eigenvalues to be printed by the eigensolver.
Then, setting \texttt{MGEigSolverSpectrum = largest\_real}, \texttt{MGEigSolverNumberOfVectors = 1} and \texttt{MGEigSolverKrylovSubspaceSize = 16} (all on the coarsest grid, the third number in the example above), one can easily obtain the largest eigenvalue of the coarse grid operator for a particular configuration or set of configurations.
Increasing this number by 20\% or so gives a good value for \texttt{MGEigSolverPolyMax}.
Subsequently, setting \texttt{MGEigSolverSpectrum = smallest\_real}, \texttt{MGEigSolverNumberOfVectors = 0, 0, 1024}, \texttt{MGEigSolverKrylovSubspaceSize = 0, 0, 3072} will give the requisite 1024 smallest eigenvalues.
Noting the largest of these, the next largest order of magnitude can be used for \texttt{MGEigSolverPolyMin}.
In other words, if the largest of these smallest eigenvalues is $4\cdot10^{-3}$, for example, then \texttt{MGEigSolverPolyMin} can be set to 0.01.
This ensures that the desired (smallest) part of the spectrum is smaller than \texttt{MGEigSolverPolyMin} and that the entire spectrum is contained in the range up to \texttt{MGEigSolverPolyMax}.
After this, polynomial acceleration can be enabled, which should reduce setup time significantly.

\subsubsection{Using the QUDA eigensolver in the HMC}

When employing the rational approximation, in order to make sure that the eigenvalue bounds are chosen appropriately, it is necessary to measure the maximal and minimal eigenvalues of the operator involved in the given monomial.
For the monomials \texttt{NDRAT, NDRATCOR, NDCLOVERRAT} and \texttt{NDCLOVERRATCOR}, this can be done using QUDA's eigensolver when, in addition to a non-zero setting for \texttt{ComputeEVFreq}, \texttt{UseExternalEigSolver = quda} is set.

The eigensolver further offers the following parameters:
\begin{itemize}
  \item{ \texttt{EigSolverPolynomialDegree}: Once appropriate parameters for the polynomial filter have been determined (see \texttt{EigSolverPolyMin} and \texttt{EigSolverPolyMax} below), when \texttt{EigSolverPolynomialDegree} is set to a non-zero value, polynomial acceleration will be used in the measurent of the smallest eigenvalue. (integer, default: \texttt{128}) }
  \item{ \texttt{EigSolverPolyMin}: Smallest eigenvalue to be excluded by the polynomial filter when polynomial acceleration is used. A good value for this should be determined by first running the eigensolver without acceleration (\texttt{EigSolverPolynomialDegree = 0}). \texttt{EigSolverPolyMin} should then be set to about $3\lambda_\mathrm{min}$. Note that this is specified in the operator normalisation, such that $\lambda_\mathrm{min}$ obtained from the measurement should be multiplied by \texttt{StildeMax} to get an appropriate value for \texttt{EigSolverPolyMin}.  (positive real number, default: \texttt{0.001})}
  \item{ \texttt{EigSolverPolyMax}: Largest eigenvalue to be excluded by the polynomial filter when polynomial acceleration is used. This should be set to a value in excess of the measured largest eigenvalue, $1.5\lambda_\mathrm{max}$, say. Note that this is specified in the operator normalisation such that the measured $\lambda_\mathrm{max}$ should be multiplied by \texttt{StildeMax} to obtain an appropriate value for \texttt{EigSolverPolyMax}. (positive real number, defaullt: \texttt{4.0})}
  \item{ \texttt{EigSolverKrylovSubspaceSize}: Size of the Krylov space used for the determination of the smallest and largest eigenvalues. The default seems to work well even for large lattices. (integer, default: \texttt{96})}
\end{itemize}


